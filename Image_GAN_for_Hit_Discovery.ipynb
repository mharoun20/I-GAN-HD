{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mPYurHjYNbQNHThAEmwVBxLDH4DPofpf",
      "authorship_tag": "ABX9TyNpVBJlAlD9Qdu8jXyzY4Yw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing "
      ],
      "metadata": {
        "id": "RPldb5g0UIzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing 0: Set up"
      ],
      "metadata": {
        "id": "_bRZkT_T1uNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Packages\n",
        "import os  #directory related\n",
        "import cv2 #work with images\n",
        "import json #work with JSON strings\n",
        "import torch #ML\n",
        "import random \n",
        "import warnings\n",
        "import numpy as np #work with arrays\n",
        "import pandas as pd #work with csv files\n",
        "from operator import itemgetter #Return a callable object that fetches item from its operand \n",
        "from torch.utils.data import Dataset #use pre-loaded datasets as well as your own data when implementing the model\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler, SequentialSampler #determine how batches should be formed.\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import rdkit #to work with ChEMBL data\n",
        "from rdkit import Chem \n",
        "from rdkit.Chem import Draw"
      ],
      "metadata": {
        "id": "qSmiQqPoUIMv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory: change to your directory if you want to replicate\n",
        "os.chdir('/content/drive/MyDrive/GANHID')\n",
        "project_file_path = '/content/drive/MyDrive/GANHID'\n",
        "training_files_path = \"{}/training_files\".format(project_file_path)\n",
        "result_files_path = \"{}/result_files\".format(project_file_path)\n",
        "trained_models_path = \"{}/trained_models\".format(project_file_path)"
      ],
      "metadata": {
        "id": "1718ODQC2BfA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-procesing 1: From Smiles to Images [D]\n"
      ],
      "metadata": {
        "id": "HYwhVj1V-UAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_images(smiles_list, output_path, img_size = (400, 400), bond_width=1.5, atom_label_font_size=55):\n",
        "    '''\n",
        "    standerdize the drawing of atoms, bonds, and labelling based on the standerdized SMILES representations \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles_list: list of SMILES strings\n",
        "    output_path: defined earlier\n",
        "    image_size: standardized \n",
        "    bond_width: standardized\n",
        "    atom_label_font_size: standerdized\n",
        "\n",
        "    '''\n",
        "\n",
        "    Draw.SetDefaultStyle(atom_label_font_size=atom_label_font_size, bondLineWidth=bond_width) # set the default atom and bond properties\n",
        "    # Iterate over the list of SMILES strings\n",
        "    for i, smiles in enumerate(smiles_list):\n",
        "        # Create the molecule object from the SMILES string\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        # Draw the molecule as an image\n",
        "        img = Draw.MolToImage(mol)\n",
        "        # Resize the image\n",
        "        img = img.resize(img_size)\n",
        "        # Save the image to a file\n",
        "        img.save(f\"{output_path}/compound_{i}.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "V1y2iZ5GFAc_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing 2: More Standerdizing and Clearning\n"
      ],
      "metadata": {
        "id": "4AtXgytEHBQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_preprocessed_bioact_file(chembl_filtered_bioact_fl, chembl_version): \n",
        "\n",
        "    '''\n",
        "    reads the raw bioactivity data, standardizes the units, and drops duplicate values \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chembl_filtered_bioact_fl: a string contains the tsv file name of the raw bioactivity data obtained from ChEMBL. \n",
        "    chembl_version: a string contains the version of the ChEMBL data that is being used\n",
        "\n",
        "    '''\n",
        "    raw_dataset_df = pd.read_csv(\"{}/{}\".format(training_files_path, chembl_filtered_bioact_fl), sep=\"\\t\", index_col=False)\n",
        "\n",
        "    # standardize units\n",
        "    raw_dataset_df.loc[raw_dataset_df['standard_units'] == 'nM', 'standard_value'] = raw_dataset_df['standard_value'] / pow(10,3)\n",
        "    raw_dataset_df.loc[raw_dataset_df['standard_units'] == 'M', 'standard_value'] = raw_dataset_df['standard_value'] / pow(10,6)\n",
        "    raw_dataset_df['standard_units'] = 'uM'\n",
        "\n",
        "    #group dataframe by target_compound_pair and standard_value, apply median and rename standard_value to median_std_val\n",
        "    raw_dataset_df['target_compound_pair'] = raw_dataset_df['Target_CHEMBL_ID']+','+raw_dataset_df['Compound_CHEMBL_ID']\n",
        "    df_median = raw_dataset_df.groupby(['target_compound_pair']).median().rename(columns={'standard_value': 'median_std_val'}).reset_index()\n",
        "\n",
        "    #merge the median dataframe with original dataframe\n",
        "    raw_dataset_df = pd.merge(raw_dataset_df, df_median[['target_compound_pair','median_std_val']],on='target_compound_pair')\n",
        "\n",
        "    # drop duplicate values with median values\n",
        "    raw_dataset_df = raw_dataset_df.loc[raw_dataset_df.duplicated(subset=['target_compound_pair'], keep=False)].sort\n",
        "\n",
        "    # drop duplicate values with median values\n",
        "    raw_dataset_df = raw_dataset_df.loc[raw_dataset_df.duplicated(subset=['target_compound_pair'], keep=False)].sort_values(by='median_std_val', ascending=True)\n",
        "    raw_dataset_df = raw_dataset_df.drop_duplicates(subset='target_compound_pair', keep='first')\n",
        "    raw_dataset_df = raw_dataset_df.drop(['standard_value'], axis=1)\n",
        "    raw_dataset_df = raw_dataset_df.rename(columns={'median_std_val': 'standard_value'})\n",
        "    \n",
        "    # write data to file\n",
        "    raw_dataset_df.to_csv(\"{}/{}_preprocessed_filtered_bioactivity_dataset.tsv\".format(training_files_path, chembl_version), sep='\\t', index=False)\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "CBcdkz-dKKCA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing 3: Filtering ChEMBL"
      ],
      "metadata": {
        "id": "gJgsEFLqRAn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uniprot_chembl_sp_id_mapping(chembl_uni_prot_mapping_fl):\n",
        "\n",
        "    '''\n",
        "    filters the dataframe to include only single protein targets \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chembl_uni_prot_mapping_fl: file location of a tab-separated file containing the mapping between UniProt IDs \n",
        "    and ChEMBL IDs for single protein targets. \n",
        "\n",
        "    '''\n",
        "    mapping_df = pd.read_csv(\"{}/{}\".format(training_files_path, chembl_uni_prot_mapping_fl), sep='\\t', header=0)\n",
        "\n",
        "    # Filter dataframe to include only single protein targets\n",
        "    mapping_df = mapping_df[mapping_df['target_type'] == 'SINGLE PROTEIN']\n",
        "\n",
        "    # Create dictionary where keys are uniprot ids and values are lists of chembl ids\n",
        "    mapping_dict = mapping_df.groupby('uniprot_id')['chembl_id'].apply(list).to_dict()\n",
        "    return mapping_dict\n"
      ],
      "metadata": {
        "id": "XPA61xV4LTAn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing 4: Building the Training set "
      ],
      "metadata": {
        "id": "6BFdDGC6USXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_act_inact_files_for_all_targets(fl, chembl_version, act_threshold, inact_threshold):\n",
        "\n",
        "    '''\n",
        "    reads a file containing compound activity data for multiple targets and filters the data \n",
        "    based on activity threshold and creates a pivot table separating the active and inactive compounds\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fl: the name of the input file containing the compound activity data\n",
        "    chembl_version: the version of the ChEMBL database used to obtain the data\n",
        "    act_threshold: a threshold value for determining active compounds\n",
        "    inact_threshold: a threshold value for determining inactive compounds\n",
        "    training_files_path: the path of the directory where the output file will be saved\n",
        "\n",
        "    '''\n",
        "    \n",
        "    pre_filt_chembl_df = pd.read_csv(\"{}/{}\".format(training_files_path, fl), sep=\"\\t\" ,index_col=False) \n",
        "    \n",
        "    # Separate the dataframe into two based on the threshold values\n",
        "    act_rows_df = pre_filt_chembl_df[pre_filt_chembl_df[\"standard_value\"] <= act_threshold]\n",
        "    inact_rows_df = pre_filt_chembl_df[pre_filt_chembl_df[\"standard_value\"] > inact_threshold]\n",
        "    \n",
        "    # Use a groupby statement and the aggregate function to group the dataframe by the target and compound IDs\n",
        "    # and then store the resulting active and inactive compounds in separate columns\n",
        "    grouped_df = act_rows_df.append(inact_rows_df).groupby(['Target_CHEMBL_ID', 'Compound_CHEMBL_ID']).agg({'standard_value':'first'}).reset_index()\n",
        "    \n",
        "    # Create a pivot table that separates the active and inactive compounds and stores them in separate columns\n",
        "    pivot_df = grouped_df.pivot_table(index='Target_CHEMBL_ID', columns='standard_value', values='Compound_CHEMBL_ID', aggfunc='first')\n",
        "    pivot_df.columns = [col if col<=act_threshold else 'inactive' for col in pivot_df.columns]\n",
        "    pivot_df.rename(columns={col: 'active' for col in pivot_df.columns if col <= act_threshold}, inplace=True)\n",
        "    \n",
        "    # Write the pivot table to a file\n",
        "    pivot_df.to_csv(\"{}/{}_preprocessed_filtered_act_inact_comps_{}_{}.tsv\".format(training_files_path, chembl_version, act_threshold, inact_threshold), sep='\\t')\n"
      ],
      "metadata": {
        "id": "4yiZwZL2QcBa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing 5: Balancing the training sets (undersampling)\n"
      ],
      "metadata": {
        "id": "xbi2rYCaXe_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def under_sample_act_inact_files(active_data, inactive_data):\n",
        "    '''\n",
        "    balance the number of samples between the two classes by under-sampling the inactive data\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    active_data: list of active samples\n",
        "    inactive_data: list of inactive samples\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    balanced_data: a list of balanced active and inactive data.\n",
        "    '''\n",
        "    \n",
        "    # Determine the number of samples to keep from the inactive class\n",
        "    n_samples_to_keep = len(active_data)\n",
        "    \n",
        "    # Randomly select a subset of the inactive class samples\n",
        "    inactive_data = random.sample(inactive_data, n_samples_to_keep)\n",
        "    \n",
        "    # Combine the active and under-sampled inactive data\n",
        "    balanced_data = active_data + inactive_data\n",
        "    \n",
        "    return balanced_data\n",
        "\n",
        "# Use the function\n",
        "#act_data = create_act_inact_files_for_all_targets(all_data, act_threshold)\n",
        "#inact_data = create_act_inact_files_for_all_targets(all_data, inact_threshold)\n",
        "#balanced_data = under_sample_act_inact_files(act_data, inact_data)\n"
      ],
      "metadata": {
        "id": "KMeJVHErXfHY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing 6: Splitting the data"
      ],
      "metadata": {
        "id": "ZaIgj_S_A3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_randomized_training_val_test_sets(neg_act_inact_fl, smiles_inchi_fl, training_files_path):\n",
        "    '''\n",
        "    create final randomized training, validation, and test sets for a given target\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    neg_act_inact_fl: a file containing a list of active and inactive compounds for each target\n",
        "    smiles_inchi_fl: a file containing the SMILES and InChI representations of compounds\n",
        "    training_files_path: path to the directory where the training files will be stored\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "\n",
        "    tar_train_val_test_dict: a dictionary with keys: training, validation, test, and the values are \n",
        "    lists of lists where the inner lists contain the compound id and label\n",
        "\n",
        "    '''\n",
        "\n",
        "    chemblid_smiles_dict = get_chemblid_smiles_inchi_dict(smiles_inchi_fl) \n",
        "    act_inact_dict = get_act_inact_list_for_all_targets(neg_act_inact_fl)\n",
        "\n",
        "    for tar in act_inact_dict:\n",
        "        os.makedirs(os.path.join(training_files_path, \"target_training_datasets\", tar, \"imgs\"), exist_ok=True)\n",
        "        act_list, inact_list = act_inact_dict[tar]\n",
        "        \n",
        "        # Trim the data to balance the number of actives and inactives\n",
        "        if len(inact_list) >= len(act_list):\n",
        "            inact_list = inact_list[:len(act_list)]\n",
        "        else:\n",
        "            act_list = act_list[:int(len(inact_list) * 1.5)]\n",
        "\n",
        "        # Shuffle the data to randomize\n",
        "        combined_list = act_list + inact_list\n",
        "        random.shuffle(combined_list)\n",
        "\n",
        "        # Define the split sizes \n",
        "        train_size = int(len(combined_list) * 0.6)\n",
        "        val_size = int(len(combined_list) * 0.2)\n",
        "\n",
        "        # Create the split\n",
        "        training_comp_id_list = combined_list[:train_size]\n",
        "        val_comp_id_list = combined_list[train_size:train_size+val_size]\n",
        "        test_comp_id_list = combined_list[train_size+val_size:]\n",
        "        \n",
        "        #Creating the lists\n",
        "        tar_train_val_test_dict = {\"training\": [], \"validation\": [], \"test\": []}\n",
        "        for split, comp_id_list in zip([\"training\", \"validation\", \"test\"], [training_comp_id_list, val_comp_id_list, test_comp_id_list]):\n",
        "            for comp_id in comp_id_list:\n",
        "                try:\n",
        "                    save_comp_imgs_from_smiles(tar, comp_id, chemblid_smiles_dict[comp_id][0])\n",
        "                    tar_train_val_test_dict[split].append([comp_id, 1])\n",
        "                except:\n",
        "                    pass\n",
        "    return tar_train_val_test_dict\n"
      ],
      "metadata": {
        "id": "pVO9Q9DOWBKW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Model"
      ],
      "metadata": {
        "id": "q5IpYaTOWBZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CAjnb9LISFY8"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32) \n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 64, 2)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(64, 32, 2)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32*5*5, fully_layer_1)\n",
        "        self.fc2 = nn.Linear(fully_layer_1, fully_layer_2)\n",
        "\n",
        "        # add last layer to produce output sample\n",
        "        self.fc3 = nn.Linear(fully_layer_2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32) \n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 64, 2)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(64, 32, 2)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32*5*5, fully_layer_1)\n",
        "        self.fc2 = nn.Linear(fully_layer_1, fully_layer_2)\n",
        "        \n",
        "        # add last layer to produce output probability\n",
        "        self.fc3 = nn.Linear(fully_layer_2, 1)\n",
        "    def forward(self, x):\n",
        "      x = self.pool(F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.2))\n",
        "      x = self.pool(F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.2))\n",
        "      x = self.pool(F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.2))\n",
        "      x = self.pool(F.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.2))\n",
        "      x = self.pool(F.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.2))\n",
        "      x = F.leaky_relu(self.fc1(x), negative_slope=0.2)\n",
        "      x = F.leaky_relu(self.fc2(x), negative_slope=0.2)\n",
        "      x = self.fc3(x)\n",
        "      \n",
        "      return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ekghr9A5gxD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training 0: Set up"
      ],
      "metadata": {
        "id": "VTLJa0rVg4Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_best_model_predictions(experiment_name, epoch, validation_scores_dict, test_scores_dict, model, project_file_path, target_id, str_arguments, all_test_comp_ids, test_labels, test_predictions):\n",
        "    '''\n",
        "    save the best model and its predictions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    experiment_name: a string representing the name of the experiment.\n",
        "    epoch: the current epoch number.\n",
        "    validation_scores_dict: a dictionary containing the scores of the model on the validation set.\n",
        "    test_scores_dict: a dictionary containing the scores of the model on the test set.\n",
        "    model: the model that needs to be saved.\n",
        "    project_file_path: the path to the project files.\n",
        "    target_id: the identifier of the target.\n",
        "    str_arguments: a string representing the arguments used to train the model.\n",
        "    all_test_comp_ids: a list of the identifiers of the compounds in the test set.\n",
        "    test_labels: a list of the labels of the compounds in the test set.\n",
        "    test_predictions: a list of the predictions of the model for the compounds in the test set. \n",
        "\n",
        "    '''\n",
        "    model_path = os.path.join(trained_models_path, experiment_name)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "\n",
        "    # Save model state dictionary\n",
        "    torch.save(model.state_dict(), os.path.join(model_path, f\"{target_id}_best_val-{str_arguments}-state_dict.pth\"))\n",
        "    \n",
        "    # Build test predictions string\n",
        "    str_test_predictions = []\n",
        "    for comp_id, label, pred in zip(all_test_comp_ids, test_labels, test_predictions):\n",
        "        str_test_predictions.append(f\"{comp_id}\\t{label}\\t{pred}\")\n",
        "    str_test_predictions = \"\\n\".join(str_test_predictions)\n",
        "\n",
        "    best_test_performance_dict = test_scores_dict\n",
        "    best_test_predictions = str_test_predictions\n",
        "    \n",
        "    # Return data\n",
        "    return validation_scores_dict, best_test_performance_dict, best_test_predictions, str_test_predictions\n"
      ],
      "metadata": {
        "id": "Yaj0ChXihB-K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training 1\n"
      ],
      "metadata": {
        "id": "l17uLPmGkeYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_val_test_loss(model, criterion, data_loader, device):\n",
        "\n",
        "  '''\n",
        "  calculates the total loss, total count, all compound ids, all labels and all predictions for a given model, criterion and data_loader\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "   model: A PyTorch model that will be used to predict the labels of the data.\n",
        "   criterion: A PyTorch criterion that will be used to calculate the loss on the predicted labels.\n",
        "   data_loader: A PyTorch data loader that will be used to iterate through the data.\n",
        "   device: A string specifying the device to be used (e.g. \"cpu\" or \"cuda\").\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  total_loss: A float representing the sum of the loss values for the entire data set.\n",
        "  total_count: An integer representing the total number of samples in the data set.\n",
        "  all_comp_ids: A list of compound ids for the entire data set.\n",
        "  all_labels: A list of labels for the entire data set.\n",
        "  all_predictions: A list of predicted labels (outputs of the model) for the entire data set. \n",
        "  '''\n",
        "    total_loss = 0.0\n",
        "    all_comp_ids = []\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for data in data_loader:\n",
        "        img_arrs, labels, comp_ids = [x.to(device) for x in data]\n",
        "        y_pred = model(img_arrs)\n",
        "        loss = criterion(y_pred.squeeze(), labels)\n",
        "        total_loss += float(loss.item())\n",
        "        all_comp_ids.extend(comp_ids)\n",
        "        _, preds = torch.max(y_pred, 1)\n",
        "        all_labels.extend(labels)\n",
        "        all_predictions.extend(preds)\n",
        "    \n",
        "    total_count = len(all_comp_ids)\n",
        "    return total_loss, total_count, all_comp_ids, all_labels, all_predictions\n"
      ],
      "metadata": {
        "id": "n1QFRsWSlgwt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training 2"
      ],
      "metadata": {
        "id": "FjHqER0Loier"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, data_loader, optimizer_G, optimizer_D, criterion, num_epochs):\n",
        "    '''\n",
        "      The function iterates through the data_loader, using each batch of data to train the discriminator and generator. \n",
        "      First, the discriminator is trained on real data, and then trained on fake data generated by the generator. \n",
        "      Then, the generator is trained to generate data that can fool the discriminator.\n",
        "      At each step, the gradients of the generator and discriminator are reset, their parameters are updated, and loss is \n",
        "      calculated and printed out to monitor the training progress.\n",
        "\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "\n",
        "      generator: a PyTorch model representing the generator network\n",
        "      discriminator: a PyTorch model representing the discriminator network\n",
        "      data_loader: a PyTorch dataloader that loads the training data\n",
        "      optimizer_G: a PyTorch optimizer for the generator network\n",
        "      optimizer_D: a PyTorch optimizer for the discriminator network\n",
        "      criterion: a PyTorch loss function to calculate the loss\n",
        "      num_epochs: an integer representing the number of times to iterate over the entire dataset during training\n",
        "\n",
        "   '''\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, data in enumerate(data_loader):\n",
        "            # 1. Train the discriminator on real data\n",
        "            optimizer_D.zero_grad()\n",
        "            real_data = data[0].to(device)\n",
        "            real_label = torch.ones(real_data.size(0), 1).to(device)\n",
        "            real_output = discriminator(real_data)\n",
        "            real_loss = criterion(real_output, real_label)\n",
        "            real_loss.backward()\n",
        "\n",
        "            # 2. Train the discriminator on fake data\n",
        "            noise = torch.randn(real_data.size(0), 100, 1, 1).to(device)\n",
        "            fake_data = generator(noise)\n",
        "            fake_label = torch.zeros(real_data.size(0), 1).to(device)\n",
        "            fake_output = discriminator(fake_data.detach())\n",
        "            fake_loss = criterion(fake_output, fake_label)\n",
        "            fake_loss.backward()\n",
        "\n",
        "            # 3. Update the discriminator's parameters\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # 4. Train the generator\n",
        "            optimizer_G.zero_grad()\n",
        "            noise = torch.randn(real_data.size(0), 100, 1, 1).to(device)\n",
        "            fake_data = generator(noise)\n",
        "            fake_output = discriminator(fake_data)\n",
        "            generator_loss = criterion(fake_output, real_label)\n",
        "            generator_loss.backward()\n",
        "\n",
        "            # 5. Update the generator's parameters\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Print Losses\n",
        "            print(\"Epoch: [%d/%d], Step: [%d/%d], D_loss: %.4f, G_loss: %.4f\" % (epoch+1, num_epochs, i+1, len(data_loader), real_loss.item() + fake_loss.item(), generator_loss.item()))\n"
      ],
      "metadata": {
        "id": "9ed4xZkEohlY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "UkIHjWuFrKhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_gan(generator, discriminator, data_loader, z_dim):\n",
        "    '''\n",
        "    evaluate the performance of the GAN model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    generator: the generator model\n",
        "    discriminator: the discriminator model\n",
        "    data_loader: a data loader for the real data\n",
        "    z_dim: the dimension of the random noise input for the generator\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    evaluation_matrix: a 2x2 matrix, where the first row represents the real data and the second row represents the fake data\n",
        "    where threshold of 0.5 is used to classify the results as real or fake\n",
        "    '''\n",
        "\n",
        "    # Initialize the evaluation matrix\n",
        "    evaluation_matrix = np.zeros((2, 2))\n",
        "    \n",
        "    # Iterate over the data\n",
        "    for real_data in data_loader:\n",
        "        # Generate fake data\n",
        "        z = torch.randn(len(real_data), z_dim)\n",
        "        fake_data = generator(z).detach()\n",
        "        \n",
        "        # Compute the discriminator's output for the real and fake data\n",
        "        real_output = discriminator(real_data)\n",
        "        fake_output = discriminator(fake_data)\n",
        "        \n",
        "        # Update the evaluation matrix\n",
        "        evaluation_matrix[0, 0] += (real_output >= 0.5).sum().item() # True Positive\n",
        "        evaluation_matrix[0, 1] += (real_output < 0.5).sum().item() # False Negative\n",
        "        evaluation_matrix[1, 0] += (fake_output < 0.5).sum().item() # False Positive\n",
        "        evaluation_matrix[1, 1] += (fake_output >= 0.5).sum().item() # True Negative\n",
        "    return evaluation_matrix\n",
        "\n",
        "#Use the function\n",
        "#data_loader = create_data_loader(data)\n",
        "#evaluation_matrix = evaluate_gan(generator, discriminator, data_loader, z_dim)\n"
      ],
      "metadata": {
        "id": "Exi5nUpY9C8W"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}